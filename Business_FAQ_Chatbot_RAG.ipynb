{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZFvlkYuudGV7pzr88E6hH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamnarendran/Business-FAQ-Chatbot-RAG/blob/main/Business_FAQ_Chatbot_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> # **Business FAQ Chatbot (RAG-based)**"
      ],
      "metadata": {
        "id": "mwjQwqybRPDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing necessary libraries\n",
        "!pip install -q chromadb\n",
        "!pip install -q sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAUd1s1xRMQq",
        "outputId": "44fc4987-f550-4f38-fe7d-e53417ed7fbf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "# Making the document chunks (Q&A style)\n",
        "sample_text = \"\"\"\n",
        "\n",
        "Q1: What services do you provide?\n",
        "A: We specialize in Business Intelligence, Data Science, Machine Learning, and Conversational AI solutions using Generative AI models.\n",
        "\n",
        "Q2: Who are your clients?\n",
        "A: Our clients include enterprises in retail, healthcare, finance, and logistics who are looking to automate insights and decision-making using AI.\n",
        "\n",
        "Q3: What technologies do you use?\n",
        "A: We utilize tools like Python, Google Cloud, TensorFlow, and various open-source LLMs including fine-tuned conversational agents.\n",
        "\n",
        "Q4: Do you provide chatbot solutions?\n",
        "A: Yes, we build intelligent chatbots using LLMs that integrate with CRMs, FAQs, internal documentation, and support systems.\n",
        "\n",
        "Q5: Where is your team located?\n",
        "A: Our headquarters is in Chennai, India, and we operate with clients globally via remote infrastructure.\n",
        "\n",
        "Q6: Can you build domain-specific LLM applications?\n",
        "A: Absolutely! We customize LLM-powered tools for retail insights, HR automation, financial report summarization, and more.\n",
        "\n",
        "Q7: Do you offer internships?\n",
        "A: Yes, we provide internships in AI/ML, LLM engineering, and data analysis â€” focused on practical, hands-on work.\n",
        "\n",
        "Q8: What industries do you specialize in?\n",
        "A: We work across diverse domains such as e-commerce, finance, education, healthcare, and logistics, delivering custom AI-powered solutions.\n",
        "\n",
        "Q9: How do you ensure data security in your AI applications?\n",
        "A: We follow industry best practices including data anonymization, secure API layers, role-based access, and compliance with GDPR/ISO standards.\n",
        "\n",
        "Q10: Can your solutions integrate with existing systems?\n",
        "A: Yes, we build flexible APIs and connectors that can integrate seamlessly with CRMs, ERPs, cloud storage, and existing enterprise infrastructure.\n",
        "\n",
        "Q11: What is your typical project timeline?\n",
        "A: Timelines vary, but POCs usually take 2â€“4 weeks, while full deployments range from 6â€“12 weeks based on scope and complexity.\n",
        "\n",
        "Q12: Do you offer post-deployment support?\n",
        "A: Absolutely. We offer ongoing maintenance, monitoring, model retraining, and performance optimization based on client needs.\n",
        "\n",
        "Q13: What kind of GenAI projects have you delivered?\n",
        "A: We've built text summarizers, business chatbots, resume reviewers, legal AI assistants, and multilingual support tools using open-source LLMs.\n",
        "\n",
        "Q14: Can your chatbots support multiple languages?\n",
        "A: Yes, our LLM-based assistants can be configured to understand and respond in English, Hindi, Tamil, and several other regional or global languages.\n",
        "\n",
        "Q15: Do you offer training or workshops for internal teams?\n",
        "A: Yes, we provide onboarding sessions, AI upskilling programs, and documentation to help your teams leverage AI solutions effectively.\n",
        "\n",
        "Q16: Can you deploy solutions on-premise?\n",
        "A: Yes, depending on client preference, we support cloud, hybrid, and fully on-prem deployments to ensure data locality and compliance.\n",
        "\n",
        "Q17: How do you price your services?\n",
        "A: Our pricing depends on project size, complexity, and engagement model (e.g., fixed, milestone-based, or retainer). We offer free consultations to scope projects.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Spliting into Q&A chunks\n",
        "chunks = [chunk.strip() for chunk in sample_text.split(\"Q\") if chunk.strip()]\n",
        "chunks = [(\"Q\" + chunk).strip() for chunk in chunks]  # Add back the 'Q'\n",
        "\n",
        "# Setup ChromaDB client (in-memory)\n",
        "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
        "collection = chroma_client.create_collection(name=\"faq_collection1\")\n",
        "\n",
        "# Loading embedding model\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Adding embedded chunks to collection\n",
        "embeddings = embed_model.encode(chunks).tolist()\n",
        "\n",
        "collection.add(\n",
        "    documents=chunks,\n",
        "    embeddings=embeddings,\n",
        "    ids=[f\"doc{i}\" for i in range(len(chunks))]\n",
        ")\n",
        "\n",
        "print(\"FAQ chunks embedded and stored successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rASm9KbkRy3e",
        "outputId": "c46f2d69-5e23-483d-927c-d6b15d72641e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAQ chunks embedded and stored successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking how our embeddings work on questions\n",
        "question = \"Who are your clients?\"\n",
        "top_k=3\n",
        "question_embedding = embed_model.encode([question]).tolist()[0]\n",
        "\n",
        "print(\"Question emdedding (Retrived questions) \\n\")\n",
        "print(question_embedding)\n",
        "\n",
        "results = collection.query(query_embeddings=[question_embedding],n_results=top_k)\n",
        "\n",
        "print(\"Retrived FAQ's \\n\")\n",
        "results['documents'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7jr-tmBaxou",
        "outputId": "a40d9cea-26a2-4852-f866-c976182982f2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question emdedding (Retrived questions) \n",
            "\n",
            "[-0.02957608550786972, 0.04186391085386276, -0.0087010208517313, -0.024030234664678574, -0.055884819477796555, 0.021520031616091728, 0.06888774037361145, -0.029949473217129707, 0.06706734746694565, -0.06529893726110458, -0.02326003834605217, 0.047076765447854996, 0.019597796723246574, 0.029591182246804237, 0.015474643558263779, 0.015786580741405487, 0.061644092202186584, -0.004915796220302582, 0.05554228648543358, -0.046077486127614975, -0.2002246230840683, -0.014997757971286774, -0.04913756623864174, 0.0034165263641625643, 0.015971951186656952, -0.04954485967755318, 0.03858521953225136, -0.012993848882615566, -0.06404611468315125, -0.04375194385647774, -0.057153962552547455, -0.02570831961929798, 0.02008519135415554, 0.02426292933523655, 0.02012987993657589, 0.0771365612745285, -0.03870512917637825, 0.03261876478791237, 0.006563838571310043, 0.03054017946124077, 0.02378295548260212, -0.002520170994102955, -0.011140587739646435, 0.0034525468945503235, 0.07860256731510162, -0.09495694935321808, 0.04502033069729805, 0.07312025129795074, -0.043034326285123825, 0.03668171912431717, -0.05396684259176254, -0.06633289158344269, -0.010838471353054047, 0.06479138880968094, -0.026096854358911514, -0.09065558016300201, -0.009831282310187817, 0.020313676446676254, -0.011808924376964569, 0.02999512478709221, -0.009879283607006073, -0.009480113163590431, -0.06273394078016281, 0.06776466965675354, 0.017645837739109993, 0.10847944766283035, -0.06946057081222534, 0.10243671387434006, -0.09990918636322021, -0.04317571222782135, -0.006282984744757414, -0.050471868366003036, 0.012922135181725025, 0.03507770597934723, 0.001985046314075589, -0.04389924928545952, 0.05089856684207916, -0.010354597121477127, 0.09819263219833374, -0.09208634495735168, 0.006461436860263348, -0.018671484664082527, -0.040268197655677795, 0.04472923278808594, -0.03307047486305237, -0.038329120725393295, 0.05714048072695732, 0.025484232231974602, 0.06107276305556297, -0.000257484323810786, 0.04287648946046829, 0.06620905548334122, -0.023119203746318817, -0.01739565283060074, -0.05212407186627388, 0.0106786685064435, 0.03814053162932396, -0.01900249719619751, -0.0674903616309166, 0.0925627052783966, -0.022990770637989044, -0.006373770069330931, 0.05953725427389145, 0.005261014681309462, -0.05040079727768898, 0.04803922027349472, -0.09506382048130035, 0.008828144520521164, -0.016495952382683754, 0.019063085317611694, -0.029566112905740738, 0.03547947108745575, -0.12282740324735641, -0.044500384479761124, 0.047123026102781296, -0.06026047468185425, -0.06229327991604805, 0.09795130789279938, 0.0032759071327745914, -0.030425872653722763, -0.027474330738186836, 0.14557182788848877, -0.03020280785858631, -0.0454329177737236, -0.03856108710169792, 0.036874402314424515, -0.014342946000397205, -5.876515195583456e-33, -0.017668725922703743, 0.09257542341947556, 0.021493347361683846, 0.1373995542526245, 0.031045973300933838, 0.014730406925082207, 0.007505524903535843, 0.05633150413632393, -0.056153617799282074, 0.0018267125124111772, 0.038726940751075745, 0.06671752035617828, 0.06367643177509308, 0.02375604771077633, -0.058980073779821396, 0.009411185048520565, 0.006123784929513931, 0.02559482492506504, -0.03657575324177742, -0.042095575481653214, -0.03366518393158913, 0.06264983117580414, -0.06723975390195847, 0.12316744029521942, 0.0031226843129843473, -0.028195777907967567, 0.02447177655994892, 0.036864761263132095, 0.05816040188074112, 0.03992151841521263, 0.07076849788427353, 0.024634353816509247, 0.037049420177936554, 0.0006264125695452094, -0.062210746109485626, 0.046969592571258545, -0.11467919498682022, -0.10881851613521576, 0.027360133826732635, -0.01336604543030262, -0.09499112516641617, 0.035868458449840546, 0.09257626533508301, -0.04169689118862152, -0.05597173422574997, -0.0010328091448172927, 0.02337603271007538, 0.01630370132625103, -0.09062647819519043, 0.06537233293056488, -0.08025312423706055, 0.020258300006389618, -0.04960079491138458, 0.1579848974943161, 0.03762757033109665, -0.06064392626285553, 0.039048295468091965, -0.06676305085420609, -0.015306946821510792, -0.02759571745991707, 0.04148830473423004, 0.032245881855487823, -0.03966427594423294, -0.04706598445773125, -0.03615748509764671, -0.013262647204101086, -0.02320927381515503, -0.04324137046933174, 0.11898249387741089, -0.038939159363508224, -0.01657872647047043, 0.04703456535935402, 0.10027766227722168, -0.01592131145298481, -0.06767266243696213, -0.00011469680612208322, -0.08341905474662781, 0.027590353041887283, -0.003594369627535343, 0.019967034459114075, -0.008308337070047855, 0.03858242556452751, 0.02741161361336708, 0.10953057557344437, -0.01692531630396843, 0.04165405407547951, -0.04518132656812668, -0.07156286388635635, -0.048441700637340546, 0.09219636023044586, -0.04108632728457451, 0.028929274529218674, 0.036342281848192215, 0.02966642752289772, -0.006935999263077974, 3.3405738454385175e-33, -0.059004466980695724, -0.032845787703990936, 0.01595265232026577, -0.02329506352543831, 0.05050244182348251, -0.06537886708974838, -0.018216494470834732, -0.01158986147493124, 0.0043634334579110146, 0.06713993102312088, -0.06856919825077057, -0.03233642876148224, -0.0631464347243309, -0.010496499948203564, -0.07395323365926743, 0.006394729018211365, -0.007858428172767162, -0.0659399926662445, 0.04891350120306015, -0.11615965515375137, 0.03385242819786072, 0.04297883063554764, 0.028306197375059128, 0.07053937017917633, 0.07246282696723938, -0.0024374662898480892, -0.0034468071535229683, -0.026891645044088364, 0.008018341846764088, -0.018908387050032616, -0.07079010456800461, 0.003078870940953493, -0.031739309430122375, -0.037114351987838745, -0.043154336512088776, 0.07351519167423248, -0.030224505811929703, 0.025229839608073235, -0.003818717086687684, -0.0333155132830143, 0.06682267040014267, -0.021556202322244644, 0.051157861948013306, 0.06288590282201767, -0.0321575291454792, -0.029296906664967537, -0.011591996066272259, -0.05166324973106384, -0.014627351425588131, -0.016000807285308838, -0.09345053881406784, 0.016670968383550644, -0.03403519466519356, -0.04676332697272301, -0.06644374877214432, 0.036872539669275284, 0.09368067979812622, -0.05019920691847801, 0.055460769683122635, 0.04392136260867119, 0.0337158627808094, 0.024536294862627983, 0.018261993303894997, 0.09997150301933289, -0.004892347380518913, -0.08837495744228363, 0.04871773347258568, 0.07345420867204666, 0.013516237027943134, -0.040257200598716736, 0.022132694721221924, -0.03839276358485222, -0.03796244412660599, -0.06364315748214722, -0.03112274594604969, -0.012142088264226913, -0.0001607568992767483, -0.05503656342625618, -0.0339500717818737, 0.013985333032906055, -0.004987572319805622, -0.05471310019493103, 0.07852468639612198, 0.12219998985528946, 0.0063197193667292595, -0.030426254495978355, 0.05871392786502838, -0.03278786316514015, 0.0011155486572533846, 0.020583493635058403, 0.015565282665193081, 0.011195363476872444, -0.0019532055594027042, -0.014282268472015858, 0.011774970218539238, -1.559291185060374e-08, -0.05617479979991913, 0.014010254293680191, 0.03184479847550392, -0.023021813482046127, -0.0110511789098382, -0.06577763706445694, 0.01223722193390131, 0.07636772096157074, -0.015815233811736107, 0.05542464554309845, 0.06564424186944962, -0.016237838193774223, -0.04800038039684296, -0.0010291824582964182, 0.0857861340045929, 0.0010034694569185376, 0.033215153962373734, 0.04337793216109276, -0.05555151775479317, -0.05509595200419426, 0.009339175187051296, 0.14174145460128784, 0.013324674218893051, -0.08129175752401352, 0.02890109270811081, 0.012706167064607143, -0.00010496170580154285, 0.06534451991319656, -0.047328632324934006, 0.06437727063894272, -0.08718762546777725, 0.051470689475536346, 0.0026963420677930117, -0.019826475530862808, 0.009911670349538326, -0.020772160962224007, -0.059854794293642044, -0.0023624012246727943, -0.013375461101531982, 0.06672737747430801, -0.061883993446826935, 0.012463855557143688, 0.020404597744345665, 0.04162687435746193, 0.027331506833434105, -0.022313391789793968, 0.02827191725373268, 0.022273343056440353, 0.004949843045324087, -0.01826326549053192, -0.0726400762796402, -0.04345189407467842, 0.10308563709259033, -0.04604062810540199, -0.03383438661694527, -0.027749035507440567, 0.021619118750095367, -0.028743641451001167, 0.036981455981731415, 0.019170882180333138, -0.03524065017700195, -0.005788929294794798, -0.05254116281867027, 0.04360669106245041]\n",
            "Retrived FAQ's \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Q2: Who are your clients?\\nA: Our clients include enterprises in retail, healthcare, finance, and logistics who are looking to automate insights and decision-making using AI.',\n",
              " 'Q17: How do you price your services?\\nA: Our pricing depends on project size, complexity, and engagement model (e.g., fixed, milestone-based, or retainer). We offer free consultations to scope projects.',\n",
              " 'Q5: Where is your team located?\\nA: Our headquarters is in Chennai, India, and we operate with clients globally via remote infrastructure.']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "def get_relevant_chunks(question, top_k=3):\n",
        "    # Embed the question\n",
        "    question_embedding = embed_model.encode([question]).tolist()[0]\n",
        "\n",
        "    # Query the ChromaDB collection\n",
        "    results = collection.query(\n",
        "        query_embeddings=[question_embedding],\n",
        "        n_results=top_k\n",
        "    )\n",
        "\n",
        "    return results['documents'][0]  # returns a list of top-k relevant chunks\n",
        "\n",
        "def generate_answer_with_context(question):\n",
        "    context_chunks = get_relevant_chunks(question, top_k=3)\n",
        "    context = \"\\n\".join(context_chunks)\n",
        "\n",
        "    prompt = f\"\"\"You are a helpful AI assistant answering questions based on a company's FAQ, answer only based on the context,\n",
        "    if the question is not in context, make response with the question they asked 'I dont have what you asked [question],please feel free to contact our mail',\n",
        "    any answer is no more than 50 words. finally say 'I'm a chatbot powered by Deepseek AI'\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    client = OpenAI(\n",
        "    base_url=\"https://api.a4f.co/v1\",\n",
        "    api_key=\"A4F-KEYS\",)  # You can get this API Keys for free from \"https://www.a4f.co/models\" and directly paste it.\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"provider-3/deepseek-v3\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "ADKZPBICSeDh"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Do you offer internships?\"\n",
        "question1 = \"Where is your office?\"\n",
        "question2 = \"how many workers in your office?\"\n",
        "question4 = \"How do you price your services?\"\n",
        "\n",
        "# Here we have asked two questions in same prompt\n",
        "question3 = \"Do you provide chatbot solutions & Can you build domain-specific LLM applications\"\n",
        "answer = generate_answer_with_context(question3)\n",
        "print(\"ğŸ’¬ Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCqBH_vsSd9F",
        "outputId": "ffbb60fd-a00c-4179-b6e6-b9661b89c387"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¬ Answer: Yes, we provide intelligent chatbot solutions and can build domain-specific LLM applications tailored to industries like retail, HR, and finance. I'm a chatbot powered by Deepseek AI.\n"
          ]
        }
      ]
    }
  ]
}